{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed():\n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "X_test = torch.Tensor( testset.data ) / 255.0# - 0.5\n",
    "y_test = torch.Tensor( testset.targets ).long()\n",
    "X_train = torch.Tensor( trainset.data ) / 255.0# - 0.5\n",
    "y_train = torch.Tensor( trainset.targets ).long()\n",
    "\n",
    "# train_dataset = TensorDataset(X_train, y_train)\n",
    "\n",
    "train_data = DataLoader(trainset, batch_size=256, shuffle=True, drop_last=True)\n",
    "test_data = DataLoader(testset, batch_size=256, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_expansion = 16\n",
    "n_latent = 16\n",
    "n_channels=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (2) must match the existing size (3) at non-singleton dimension 1.  Target sizes: [10, 2].  Tensor sizes: [10, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pe_digit\n\u001b[1;32m     19\u001b[0m temp \u001b[38;5;241m=\u001b[39m SinePositionalEncoding(\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mtemp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m, in \u001b[0;36mSinePositionalEncoding.forward\u001b[0;34m(self, digit)\u001b[0m\n\u001b[1;32m     10\u001b[0m pe \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model)\n\u001b[1;32m     11\u001b[0m pe[:, \u001b[38;5;241m0\u001b[39m::\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msin(position \u001b[38;5;241m*\u001b[39m div_term)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mpe\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcos(position \u001b[38;5;241m*\u001b[39m div_term)\n\u001b[1;32m     13\u001b[0m pe \u001b[38;5;241m=\u001b[39m pe\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Get the positional encoding for the given digit\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (2) must match the existing size (3) at non-singleton dimension 1.  Target sizes: [10, 2].  Tensor sizes: [10, 3]"
     ]
    }
   ],
   "source": [
    "class SinePositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(SinePositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, digit):\n",
    "        # Compute the positional encodings for the given digit\n",
    "        position = torch.arange(0, 10).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2) * -(math.log(10000.0) / self.d_model))\n",
    "        pe = torch.zeros(10, self.d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # Get the positional encoding for the given digit\n",
    "        pe_digit = pe[:, digit, :].unsqueeze(0)\n",
    "        return pe_digit\n",
    "\n",
    "temp = SinePositionalEncoding(5)\n",
    "\n",
    "temp(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=n_latent, out_channels=n_expansion*8, kernel_size=4, stride=1, bias=True),\n",
    "            nn.BatchNorm2d(n_expansion*8),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=n_expansion*8, out_channels=n_expansion*4, kernel_size=4, stride=2, bias=True),\n",
    "            nn.BatchNorm2d(n_expansion*4),\n",
    "            nn.ReLU(),\n",
    "\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=n_expansion*4, out_channels=n_expansion*2, kernel_size=4, stride=2, bias=True),\n",
    "            nn.BatchNorm2d(n_expansion*2),\n",
    "            nn.ReLU(),\n",
    "\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=n_expansion*2, out_channels=n_expansion, kernel_size=4, stride=1, bias=True),\n",
    "            nn.BatchNorm2d(n_expansion),\n",
    "            nn.ReLU(),\n",
    "\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=n_expansion, out_channels=1, kernel_size=4, stride=1, bias=True),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)\n",
    "    \n",
    "def generate_samples(batch_size=256):\n",
    "    return torch.randn((batch_size, n_latent, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=n_channels, out_channels=n_expansion, kernel_size=4, stride=1, padding=0, bias=True),\n",
    "\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=n_expansion, out_channels=n_expansion*2, kernel_size=4, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(n_expansion*2),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=n_expansion*2, out_channels=n_expansion*4, kernel_size=4, stride=2, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(n_expansion*4),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=n_expansion*4, out_channels=n_expansion*8, kernel_size=4, stride=2, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(n_expansion*8),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=n_expansion*8, out_channels=1, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.Sigmoid()  \n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(D, G, lr, batch_size, tolerance, patience, lim):\n",
    "    loss_function = nn.BCELoss()\n",
    "    \n",
    "    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    N_train = len(trainset)\n",
    "    N_test = len(testset)\n",
    "\n",
    "    N_batches_train = N_train // batch_size\n",
    "    N_batches_test = N_test // batch_size\n",
    "\n",
    "    optimizer_G = optim.Adam(G.parameters(), lr=lr)\n",
    "    optimizer_D = optim.Adam(D.parameters(), lr=lr)\n",
    "\n",
    "    for i in range(lim):\n",
    "\n",
    "        # Train discriminator\n",
    "\n",
    "        train_epoch_D_loss = 0\n",
    "        train_epoch_D_correct = 0\n",
    "        train_epoch_D_attempted = 0\n",
    "\n",
    "        train_epoch_G_loss = 0\n",
    "        \n",
    "        for X_batch0, _ in train_loader:\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            y_batch0 = torch.zeros(batch_size)\n",
    "\n",
    "            # Do not record generator gradients during discriminator training\n",
    "            with torch.no_grad():\n",
    "                X_batch1 = G(generate_samples(batch_size=batch_size))\n",
    "            y_batch1 = torch.ones(batch_size)\n",
    "\n",
    "            X_batch = torch.cat([X_batch0, X_batch1])\n",
    "            y_batch = torch.cat([y_batch0, y_batch1])\n",
    "\n",
    "            # Note: D(X_batch) has dims (batch_size, 1, 1, 1)\n",
    "            y_batch_probs = torch.squeeze(D(X_batch))\n",
    "            y_batch_preds = torch.round(y_batch_probs)\n",
    "            \n",
    "            loss_batch_D = loss_function(y_batch_probs, y_batch)\n",
    "\n",
    "            loss_batch_D.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            train_epoch_D_loss += loss_batch_D.item()\n",
    "\n",
    "            train_epoch_D_attempted += y_batch.shape[0]\n",
    "            train_epoch_D_correct += accuracy_score(y_batch.detach().numpy(), y_batch_preds.detach().numpy(), normalize=False)\n",
    "\n",
    "            print(f\"Epoch 1 D: {loss_batch_D:.3f} {accuracy_score(y_batch.detach().numpy(), y_batch_preds.detach().numpy())}\")\n",
    "\n",
    "            # Train generator\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            X_batch = G(generate_samples(batch_size=batch_size*2))\n",
    "            y_batch = torch.ones(batch_size*2)\n",
    "\n",
    "            y_batch_probs = torch.squeeze(D(X_batch))\n",
    "            y_batch_preds = torch.round(y_batch_probs)\n",
    "\n",
    "            loss_batch_G = -loss_function(y_batch_probs, y_batch)\n",
    "\n",
    "            loss_batch_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            train_epoch_G_loss += loss_batch_G.item()\n",
    "\n",
    "            print(f\"Epoch 1 G: {loss_batch_G:.3f} {accuracy_score(y_batch.detach().numpy(), y_batch_preds.detach().numpy())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=.001\n",
    "batch_size=256\n",
    "tolerance=.01\n",
    "patience=10\n",
    "lim=1\n",
    "\n",
    "hyperparams = {\n",
    "    'lr': lr,\n",
    "    'batch_size': batch_size,\n",
    "    'tolerance': tolerance,\n",
    "    'patience': patience,\n",
    "    'lim': lim,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (layers): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(16, 32, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): ReLU()\n",
       "    (5): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): Conv2d(128, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (12): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create models\n",
    "G = Generator()\n",
    "D = Discriminator()\n",
    "\n",
    "# Randomly initialize model weights\n",
    "G.apply(weights_init)\n",
    "D.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 D: 0.848 0.412109375\n",
      "Epoch 1 G: -0.553 0.751953125\n",
      "Epoch 1 D: 0.531 0.689453125\n",
      "Epoch 1 G: -1.174 0.4296875\n",
      "Epoch 1 D: 0.533 0.751953125\n",
      "Epoch 1 G: -1.317 0.5703125\n",
      "Epoch 1 D: 0.517 0.78125\n",
      "Epoch 1 G: -1.076 0.66796875\n",
      "Epoch 1 D: 0.367 0.876953125\n",
      "Epoch 1 G: -0.949 0.69921875\n",
      "Epoch 1 D: 0.320 0.876953125\n",
      "Epoch 1 G: -0.906 0.671875\n",
      "Epoch 1 D: 0.172 0.94140625\n",
      "Epoch 1 G: -0.995 0.65625\n",
      "Epoch 1 D: 0.067 0.998046875\n",
      "Epoch 1 G: -1.549 0.359375\n",
      "Epoch 1 D: 0.052 0.998046875\n",
      "Epoch 1 G: -2.319 0.125\n",
      "Epoch 1 D: 0.059 0.99609375\n",
      "Epoch 1 G: -2.907 0.078125\n",
      "Epoch 1 D: 0.064 0.998046875\n",
      "Epoch 1 G: -3.444 0.091796875\n",
      "Epoch 1 D: 0.067 1.0\n",
      "Epoch 1 G: -3.936 0.150390625\n",
      "Epoch 1 D: 0.066 0.994140625\n",
      "Epoch 1 G: -4.153 0.158203125\n",
      "Epoch 1 D: 0.050 0.998046875\n",
      "Epoch 1 G: -4.291 0.22265625\n",
      "Epoch 1 D: 0.029 0.994140625\n",
      "Epoch 1 G: -4.357 0.23828125\n",
      "Epoch 1 D: 0.022 0.994140625\n",
      "Epoch 1 G: -4.520 0.236328125\n",
      "Epoch 1 D: 0.015 0.994140625\n",
      "Epoch 1 G: -4.702 0.240234375\n",
      "Epoch 1 D: 0.009 1.0\n",
      "Epoch 1 G: -4.914 0.26171875\n",
      "Epoch 1 D: 0.007 1.0\n",
      "Epoch 1 G: -5.159 0.267578125\n",
      "Epoch 1 D: 0.005 1.0\n",
      "Epoch 1 G: -5.537 0.265625\n",
      "Epoch 1 D: 0.005 1.0\n",
      "Epoch 1 G: -5.905 0.236328125\n",
      "Epoch 1 D: 0.003 1.0\n",
      "Epoch 1 G: -6.198 0.23828125\n",
      "Epoch 1 D: 0.002 1.0\n",
      "Epoch 1 G: -6.523 0.208984375\n",
      "Epoch 1 D: 0.004 1.0\n",
      "Epoch 1 G: -6.899 0.14453125\n",
      "Epoch 1 D: 0.002 1.0\n",
      "Epoch 1 G: -7.173 0.08203125\n",
      "Epoch 1 D: 0.003 1.0\n",
      "Epoch 1 G: -6.854 0.07421875\n",
      "Epoch 1 D: 0.002 1.0\n",
      "Epoch 1 G: -7.108 0.109375\n",
      "Epoch 1 D: 0.002 1.0\n",
      "Epoch 1 G: -7.748 0.10546875\n",
      "Epoch 1 D: 0.001 1.0\n",
      "Epoch 1 G: -7.594 0.107421875\n",
      "Epoch 1 D: 0.002 1.0\n",
      "Epoch 1 G: -8.100 0.1171875\n",
      "Epoch 1 D: 0.002 1.0\n",
      "Epoch 1 G: -8.420 0.119140625\n",
      "Epoch 1 D: 0.002 1.0\n",
      "Epoch 1 G: -8.441 0.080078125\n",
      "Epoch 1 D: 0.001 1.0\n",
      "Epoch 1 G: -8.980 0.064453125\n",
      "Epoch 1 D: 0.001 1.0\n",
      "Epoch 1 G: -8.724 0.0546875\n",
      "Epoch 1 D: 0.001 1.0\n",
      "Epoch 1 G: -9.314 0.076171875\n",
      "Epoch 1 D: 0.002 1.0\n",
      "Epoch 1 G: -9.219 0.078125\n",
      "Epoch 1 D: 0.001 1.0\n",
      "Epoch 1 G: -9.489 0.080078125\n",
      "Epoch 1 D: 0.001 1.0\n",
      "Epoch 1 G: -9.760 0.0703125\n",
      "Epoch 1 D: 0.001 1.0\n",
      "Epoch 1 G: -9.845 0.04296875\n",
      "Epoch 1 D: 0.001 1.0\n",
      "Epoch 1 G: -10.157 0.037109375\n",
      "Epoch 1 D: 0.001 1.0\n",
      "Epoch 1 G: -10.087 0.033203125\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -10.430 0.025390625\n",
      "Epoch 1 D: 0.001 1.0\n",
      "Epoch 1 G: -10.515 0.01953125\n",
      "Epoch 1 D: 0.001 1.0\n",
      "Epoch 1 G: -10.798 0.015625\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -10.842 0.009765625\n",
      "Epoch 1 D: 0.001 1.0\n",
      "Epoch 1 G: -11.107 0.0078125\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -11.129 0.0078125\n",
      "Epoch 1 D: 0.001 1.0\n",
      "Epoch 1 G: -11.236 0.00390625\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -11.341 0.005859375\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -11.082 0.005859375\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -11.455 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -11.807 0.00390625\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -11.713 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -11.828 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -12.083 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -12.083 0.005859375\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -12.144 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -12.517 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -12.232 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -12.651 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -12.665 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -12.739 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -12.884 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -12.879 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -12.939 0.001953125\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -12.894 0.001953125\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -13.024 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -13.391 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -13.294 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -13.287 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -13.490 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -13.536 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -13.617 0.001953125\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -13.582 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -13.902 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -13.657 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -13.701 0.0\n",
      "Epoch 1 D: 0.001 1.0\n",
      "Epoch 1 G: -14.022 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -13.801 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -14.002 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -13.995 0.0\n",
      "Epoch 1 D: 0.001 1.0\n",
      "Epoch 1 G: -14.111 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -13.833 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -14.038 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -14.070 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -14.008 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -14.479 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -14.316 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -14.452 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -14.416 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -14.472 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -14.477 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -14.767 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -14.476 0.0\n",
      "Epoch 1 D: 0.001 1.0\n",
      "Epoch 1 G: -14.640 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -14.748 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -14.751 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -14.704 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -14.865 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -14.837 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -14.572 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -14.810 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -14.664 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -14.896 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.068 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.003 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.019 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.183 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.113 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.074 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.324 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.123 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.191 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.352 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.295 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.181 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.399 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.305 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.215 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.438 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.345 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.305 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.566 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.529 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.455 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.587 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.435 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.262 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.591 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.525 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.454 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.717 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.378 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.479 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.662 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.501 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.736 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.723 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.725 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.787 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.747 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.852 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.886 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.868 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.872 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.842 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.840 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.754 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.952 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.901 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.968 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.821 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.705 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.988 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.092 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.957 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.092 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -15.993 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.073 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.185 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.139 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.063 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.171 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.079 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.118 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.236 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.290 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.293 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.315 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.228 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.228 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.276 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.348 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.392 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.381 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.271 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.316 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.327 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.142 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.456 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.392 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.217 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.175 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.375 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.383 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.378 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.135 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.446 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.395 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.226 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.398 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.296 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.263 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.435 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.385 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.395 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.526 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.371 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.418 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.538 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.384 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.489 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.545 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.455 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.652 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.552 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.640 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.659 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.663 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.659 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.682 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.655 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.661 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.702 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.648 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.554 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.599 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.630 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.668 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.679 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.649 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.627 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.774 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.750 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.775 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.652 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.663 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.656 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.809 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.761 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.784 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.739 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.800 0.0\n",
      "Epoch 1 D: 0.000 1.0\n",
      "Epoch 1 G: -16.670 0.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([512])) that is different to the input size (torch.Size([352])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[147], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_gan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[144], line 45\u001b[0m, in \u001b[0;36mtrain_gan\u001b[0;34m(D, G, lr, batch_size, tolerance, patience, lim)\u001b[0m\n\u001b[1;32m     42\u001b[0m y_batch_probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqueeze(D(X_batch))\n\u001b[1;32m     43\u001b[0m y_batch_preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mround(y_batch_probs)\n\u001b[0;32m---> 45\u001b[0m loss_batch_D \u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_batch_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m loss_batch_D\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     48\u001b[0m optimizer_D\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/ml_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml_env/lib/python3.8/site-packages/torch/nn/modules/loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml_env/lib/python3.8/site-packages/torch/nn/functional.py:3118\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3116\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[0;32m-> 3118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3120\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m   3121\u001b[0m     )\n\u001b[1;32m   3123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3124\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([512])) that is different to the input size (torch.Size([352])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "train_gan(D, G, **hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
